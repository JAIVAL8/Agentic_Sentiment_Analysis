{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c230e66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, re, getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c03947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from pydantic import BaseModel, Field\n",
    "from linkup import LinkupClient\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai.tools import BaseTool\n",
    "\n",
    "\n",
    "def get_llm_client():\n",
    "    \"\"\"Initialize and return the LLM client\"\"\"\n",
    "    return LLM(model=\"openai/gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f56ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Linkup Search Tool\n",
    "class LinkupSearchInput(BaseModel):\n",
    "    \"\"\"Input schema for Linkup Search Tool.\"\"\"\n",
    "    query: str = Field(description=\"The search query to perform\")\n",
    "    depth: str = Field(default=\"standard\",\n",
    "                       description=\"Depth of search: 'standard' or 'deep'\")\n",
    "    output_type: str = Field(\n",
    "        default=\"searchResults\", description=\"Output type: 'searchResults', 'sourcedAnswer', or 'structured'\")\n",
    "\n",
    "class LinkupSearchTool(BaseTool):\n",
    "    name: str = \"Linkup Search\"\n",
    "    description: str = \"Search the web for information using Linkup and return comprehensive results\"\n",
    "    args_schema: Type[BaseModel] = LinkupSearchInput\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _run(self, query: str, depth: str = \"standard\", output_type: str = \"searchResults\") -> str:\n",
    "        \"\"\"Execute Linkup search and return results.\"\"\"\n",
    "        try:\n",
    "            # Initialize Linkup client with API key from environment variables\n",
    "            linkup_client = LinkupClient(api_key=os.getenv(\"LINKUP_API_KEY\"))\n",
    "\n",
    "            # Perform search\n",
    "            search_response = linkup_client.search(\n",
    "                query=query,\n",
    "                depth=depth,\n",
    "                output_type=output_type\n",
    "            )\n",
    "\n",
    "            return str(search_response)\n",
    "        except Exception as e:\n",
    "            return f\"Error occurred while searching: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd212f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from youtube_search import YoutubeSearch\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_comment_downloader import YoutubeCommentDownloader, SORT_BY_POPULAR\n",
    "from itertools import islice\n",
    "\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"Extract the video ID from YouTube URL.\"\"\"\n",
    "    if \"v=\" in url:\n",
    "        return url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    elif \"youtu.be/\" in url:\n",
    "        return url.split(\"youtu.be/\")[-1].split(\"?\")[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_transcript_text(video_url: str) -> str:\n",
    "    \"\"\"Fetch English transcript and return as plain text.\"\"\"\n",
    "    try:\n",
    "        video_id = extract_video_id(video_url)\n",
    "        if not video_id:\n",
    "            return \"Invalid YouTube URL.\"\n",
    "\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "        transcript = ytt_api.fetch(video_id)  # Correct up-to-date method\n",
    "        # Combine transcript text\n",
    "        full_text = \" \".join([snippet.text for snippet in transcript])\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        return f\"Transcript error: {e}\"\n",
    "\n",
    "\n",
    "def get_top_comments(video_url: str, max_comments: int = 50) -> str:\n",
    "    \"\"\"Fetch top YouTube comments (up to `max_comments`).\"\"\"\n",
    "    try:\n",
    "        video_id = extract_video_id(video_url)\n",
    "        if not video_id:\n",
    "            return \"Invalid YouTube URL.\"\n",
    "\n",
    "        downloader = YoutubeCommentDownloader()\n",
    "        comments_iter = downloader.get_comments_from_url(\n",
    "            f\"https://www.youtube.com/watch?v={video_id}\",\n",
    "            sort_by=SORT_BY_POPULAR\n",
    "        )\n",
    "\n",
    "        texts = [c.get(\"text\", \"\").strip() for c in islice(comments_iter, max_comments) if c.get(\"text\")]\n",
    "        return \"\\n\\n\".join(texts) if texts else \"No comments found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Comments error: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "class YouTubeSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query for YouTube\")\n",
    "\n",
    "\n",
    "class YouTube_Search_Tool(BaseTool):\n",
    "    name: str = \"YouTube Search\"\n",
    "    description: str = \"Searches YouTube and returns video metadata and engagement info\"\n",
    "    args_schema: Type[BaseModel] = YouTubeSearchInput \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            results = json.loads(YoutubeSearch(query, max_results=10).to_json())\n",
    "            video_summaries = []\n",
    "\n",
    "            for video in results.get(\"videos\", []):\n",
    "                title = video.get(\"title\")\n",
    "                channel = video.get(\"channel\")\n",
    "                duration = video.get(\"duration\")\n",
    "                views = video.get(\"views\")\n",
    "                publish_time = video.get(\"publish_time\")\n",
    "                url_suffix = video.get(\"url_suffix\")\n",
    "\n",
    "                full_url = f\"https://www.youtube.com{url_suffix}\"\n",
    "                \n",
    "                if \"/shorts/\" in full_url:\n",
    "                    continue\n",
    "\n",
    "                summary = f\"\"\"Title       : {title}\n",
    "                                Channel     : {channel}\n",
    "                                Duration    : {duration}\n",
    "                                Views       : {views}\n",
    "                                Published   : {publish_time}\n",
    "                                URL         : {full_url}\n",
    "\n",
    "                                Transcript  : {get_transcript_text(full_url)}\n",
    "\n",
    "                                Top Comments:\n",
    "                                {get_top_comments(full_url)}\n",
    "                                {'-' * 80}\n",
    "                                                \"\"\".strip()\n",
    "\n",
    "                video_summaries.append(summary)\n",
    "\n",
    "            return str(\"\\n\\n\".join(video_summaries))\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error during YouTube search: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b82c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomberg_sov_crew(query: str, top_n: int = 10):\n",
    "    \"\"\"Create CrewAI agents and tasks for Share of Voice analysis across Web and YouTube.\"\"\"\n",
    "\n",
    "    # Tools\n",
    "    linkup_search_tool = LinkupSearchTool()\n",
    "    youtube_search_tool = YouTube_Search_Tool()\n",
    "    llm = get_llm_client()\n",
    "\n",
    "    # Agents\n",
    "    search_agent = Agent(\n",
    "        role=\"Web Searcher\",\n",
    "        goal=\"Collect top-N search results from web sources for a given query.\",\n",
    "        backstory=\"Expert in search engines like Google, X, Instagram, and Reddit.\",\n",
    "        tools=[linkup_search_tool],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    youtube_agent = Agent(\n",
    "        role=\"YouTube Researcher\",\n",
    "        goal=\"Find top YouTube videos, transcripts, and comments for a given query.\",\n",
    "        backstory=\"Specialist in sourcing relevant YouTube videos, transcripts, and audience sentiment.\",\n",
    "        tools=[youtube_search_tool],\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    sov_analyst = Agent(\n",
    "        role=\"SoV Analyst\",\n",
    "        goal=\"Quantify Atomberg's Share of Voice (mentions, sentiment, engagement) across web and YouTube.\",\n",
    "        backstory=\"Skilled at entity recognition, sentiment analysis, and competitive benchmarking.\",\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    insight_agent = Agent(\n",
    "        role=\"Insight Synthesizer\",\n",
    "        goal=\"Generate strategic insights from Share of Voice data across all platforms.\",\n",
    "        backstory=\"Combines analytical insight with strategic vision to guide marketing direction.\",\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    writer_agent = Agent(\n",
    "        role=\"Marketing Report Writer\",\n",
    "        goal=\"Write a comprehensive markdown report with insights, SoV tables, and recommendations.\",\n",
    "        backstory=\"Crafts engaging and data-driven briefs for internal teams.\",\n",
    "        llm=llm,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Tasks\n",
    "    search_task = Task(\n",
    "        description=f\"Use Linkup to fetch top {top_n} Google/Reddit/Instagram results for query: '{query}'. Return titles, snippets, URLs, and any engagement metrics.\",\n",
    "        agent=search_agent,\n",
    "        tools=[linkup_search_tool],\n",
    "        expected_output=\"Top web results with metadata and engagement signals.\",\n",
    "    )\n",
    "\n",
    "    youtube_search_task = Task(\n",
    "        description=f\"Use YouTubeSearchTool to fetch top {top_n} YouTube videos for query: '{query}'. Return title, channel, URL, transcript, and top comments.\",\n",
    "        agent=youtube_agent,\n",
    "        tools=[youtube_search_tool],\n",
    "        expected_output=\"Top YouTube video summaries with transcript and audience comments.\",\n",
    "    )\n",
    "\n",
    "    analysis_task = Task(\n",
    "        description=\"Analyze both web and YouTube results. Identify Atomberg and competitor mentions, perform sentiment and engagement analysis. Compute Atomberg's Share of Voice (SoV).\",\n",
    "        agent=sov_analyst,\n",
    "        context=[search_task, youtube_search_task],\n",
    "        expected_output=\"Table comparing SoV across platforms, with sentiment and engagement metrics.\",\n",
    "    )\n",
    "\n",
    "    insight_task = Task(\n",
    "        description=\"Synthesize key findings: where Atomberg leads or lags, content opportunities, competitor strengths, and platform-specific patterns.\",\n",
    "        agent=insight_agent,\n",
    "        context=[analysis_task],\n",
    "        expected_output=\"Strategic recommendations with insights across platforms and keywords.\",\n",
    "    )\n",
    "\n",
    "    report_task = Task(\n",
    "        description=\"Write a markdown report including SoV table, insights, platform trends, and action recommendations.\",\n",
    "        agent=writer_agent,\n",
    "        context=[insight_task],\n",
    "        expected_output=\"Final markdown marketing brief with citations and takeaways.\",\n",
    "    )\n",
    "\n",
    "    crew = Crew(\n",
    "        agents=[search_agent, youtube_agent, sov_analyst, insight_agent, writer_agent],\n",
    "        tasks=[search_task, youtube_search_task, analysis_task, insight_task, report_task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return crew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"smart fan\"\n",
    "crew = create_atomberg_sov_crew(query, top_n=10)\n",
    "result = crew.kickoff()\n",
    "#print(result.raw)\n",
    "with open('markdown.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(result.raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1713039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] miktex-dvipdfmx: major issue: So far, no MiKTeX administrator has checked for updates.\n",
      "xelatex: major issue: So far, no MiKTeX administrator has checked for updates.\n",
      "miktex-dvipdfmx: major issue: So far, no MiKTeX administrator has checked for updates.\n",
      "xelatex: major issue: So far, no MiKTeX administrator has checked for updates.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypandoc\n",
    "\n",
    "pypandoc.convert_file(\n",
    "    'markdown.md',\n",
    "    'pdf',\n",
    "    outputfile='output.pdf',\n",
    "    extra_args=[\n",
    "        '--standalone',\n",
    "        '--pdf-engine=xelatex',  # or lualatex\n",
    "        '--variable', 'geometry:margin=1in',  # you can make this smaller\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
